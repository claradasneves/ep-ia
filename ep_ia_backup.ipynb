{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGUvj5itk-3m"
      },
      "source": [
        "# Trabalho de IA: MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuOcZ8Nto_bY"
      },
      "source": [
        "Integrantes:\n",
        "- Ana Clara das Neves Barreto - 13672540\n",
        "- Eloisa Antero Guisse - 13781924\n",
        "- Marcos Martins de Oliveira Pacheco - 13672602\n",
        "- Jamyle Gonçalves Rodrigues Silva -\n",
        "- Lucca Pinto -\n",
        "- Sarah Klock Mauricio - 13673131\n",
        "\n",
        "Divas pop q nao tem hit solo ha 50 anos:\n",
        "- Lady Gaga\n",
        "- Katy Perry\n",
        "- Demi Lovato\n",
        "- Selena Gomez\n",
        "- Jessie J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN8ONKTblF9z"
      },
      "source": [
        "## Desenvolvimento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uRTRIhbRmLbE"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import math\n",
        "from graphviz import Digraph\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funções que lidam com a conversão entre letras e vetores de binários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_0sIq_i-l53g"
      },
      "outputs": [],
      "source": [
        "alfabeto = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "def letra_para_vetor(letra):\n",
        "    \"\"\"\n",
        "    Transforma um caractere num vetor de 26 posições, no qual a posição da letra correspondente é 1 e o restante é -1.\n",
        "    :param letra: Letra a ser transformada num vetor\n",
        "    :return: Vetor binário de 26 posições\n",
        "    \"\"\"\n",
        "    vetor = [-1] * 26\n",
        "    pos_letra = alfabeto.index(letra)\n",
        "    vetor[pos_letra] = 1\n",
        "\n",
        "    return vetor\n",
        "\n",
        "def vetor_para_letra(vetor):\n",
        "    \"\"\"\n",
        "    Encontra a letra do alfabeto correspondente a um vetor de 26 posições.\n",
        "    :param vetor: Vetor de 26 posições que representa uma letra do alfabeto.\n",
        "    :return: Letra correspondente.\n",
        "    \"\"\"\n",
        "    # Encontra a posição do maior valor no vetor\n",
        "    maior_pos = np.argmax(vetor)\n",
        "\n",
        "    # Encontra a letra correspondente no alfabeto\n",
        "    letra = alfabeto[maior_pos]\n",
        "\n",
        "    return letra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funções para leituras de arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ler_arq_imagens(nome_arq):\n",
        "    \"\"\"\n",
        "    Lê o arquivo de imagens e retorna um array numpy.\n",
        "\n",
        "    Args:\n",
        "        nome_arq (str): Caminho para o arquivo de imagens.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array numpy contendo as imagens.\n",
        "    \"\"\"\n",
        "    with open(nome_arq, 'r') as file:\n",
        "        data = file.read().strip().split('\\n')\n",
        "    data = [list(map(int, filter(lambda x: x.strip(), line.split(',')))) for line in data if line.strip()]\n",
        "    return np.array(data)\n",
        "\n",
        "def ler_arq_classes(nome_arq):\n",
        "    \"\"\"\n",
        "    Lê o arquivo de classes e retorna um array numpy.\n",
        "\n",
        "    Args:\n",
        "        nome_arq (str): Caminho para o arquivo de classes.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array numpy contendo as classes.\n",
        "    \"\"\"\n",
        "    with open(nome_arq, 'r') as file:\n",
        "        data = []\n",
        "        for line in file:\n",
        "            vetor_letra = letra_para_vetor(line[0])\n",
        "            data.append(vetor_letra)\n",
        "    return np.array(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funções para a divisão dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shuffle_dados(dados, classes):\n",
        "    \"\"\"\n",
        "        Embaralha os dados de um vetor mantendo as respectivas classificações na mesma ordem.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Vetor dos dados a serem divididos.\n",
        "            classes (np.array): Classificação dos dados a serem dividos.\n",
        "\n",
        "        Returns:\n",
        "            (np.array): Vetor de dados embaralhado.\n",
        "            (np.array): Vetor de classificação dos dados da primeira parte da divisão.\n",
        "        \"\"\"\n",
        "    indices = np.arange(dados.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    return dados[indices], classes[indices]\n",
        "\n",
        "def dividir_dados(entrada, classes, test_proportion=0.2):\n",
        "    \"\"\"\n",
        "        Divide dois vetores de dados em uma determinada proporção (padrão: 80/20).\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Vetor dos dados a serem divididos.\n",
        "            classes (np.array): Classificação dos dados a serem dividos.\n",
        "            div_proportion (float): Proporção da divisão dos dados (padrão: 20%)\n",
        "\n",
        "        Returns:\n",
        "            dados_primeira_parte (np.array): Vetor de dados da primeira parte da divisão.\n",
        "            classes_primeira_parte (np.array): Vetor de classificação dos dados da primeira parte da divisão.\n",
        "            dados_segunda_parte (np.array): Vetor de dados da segunda parte da divisão.\n",
        "            classes_segunda_parte (np.array): Vetor de classificação dos dados da segunda parte da divisão.\n",
        "        \"\"\"\n",
        "    # Transforma os vetores das classes nas letras correspondentes\n",
        "    classes_letras = []\n",
        "    for elemento in classes:\n",
        "        classes_letras.append(vetor_para_letra(elemento))\n",
        "\n",
        "    classes_letras = np.array(classes_letras)\n",
        "\n",
        "    # Conta o número de classes\n",
        "    unique_classes, class_counts = np.unique(classes_letras, return_counts=True)\n",
        "    \n",
        "    # Calcula a quantidade desejada de cada classe no primeiro conjunto\n",
        "    desired_counts = (class_counts * (1 - test_proportion)).astype(int)\n",
        "    \n",
        "    # Inicializa os vetores para as duas partes\n",
        "    dados_primeira_parte = []\n",
        "    classes_primeira_parte = []\n",
        "    dados_segunda_parte = []\n",
        "    classes_segunda_parte = []\n",
        "    \n",
        "    for classe, desired_count in zip(unique_classes, desired_counts):\n",
        "        # Índices das amostras da classe atual\n",
        "        indices_classe = np.where(classes_letras == classe)[0]\n",
        "\n",
        "        # Seleciona aleatoriamente as amostras para o primeiro conjunto\n",
        "        selected_indices = np.random.choice(indices_classe, size=desired_count, replace=False)\n",
        "        \n",
        "        # Divide os dados e classes\n",
        "        dados_primeira_parte.append(entrada[selected_indices])\n",
        "        classes_primeira_parte.append(classes[selected_indices])\n",
        "        \n",
        "        # As amostras restantes vão para o segundo conjunto\n",
        "        remaining_indices = np.setdiff1d(indices_classe, selected_indices)\n",
        "        dados_segunda_parte.append(entrada[remaining_indices])\n",
        "        classes_segunda_parte.append(classes[remaining_indices])\n",
        "    \n",
        "    # Concatena os vetores\n",
        "    dados_primeira_parte = np.concatenate(dados_primeira_parte)\n",
        "    classes_primeira_parte = np.concatenate(classes_primeira_parte)\n",
        "    dados_segunda_parte = np.concatenate(dados_segunda_parte)\n",
        "    classes_segunda_parte = np.concatenate(classes_segunda_parte)\n",
        "\n",
        "    dados_primeira_parte, classes_primeira_parte = shuffle_dados(dados_primeira_parte, classes_primeira_parte)\n",
        "    dados_segunda_parte, classes_segunda_parte = shuffle_dados(dados_segunda_parte, classes_segunda_parte)\n",
        "\n",
        "    return dados_primeira_parte, classes_primeira_parte, dados_segunda_parte, classes_segunda_parte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-hoEUDxPfc4"
      },
      "source": [
        "Funcao pra reconstruir imagens a partir do array de array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "nD_uUiPPEYlC",
        "outputId": "a9db9514-13b0-4b09-d43e-b36ca64d539c"
      },
      "outputs": [],
      "source": [
        "def reconstruct_image(row_index, image_array):\n",
        "    \"\"\"\n",
        "    Reconstrói e plota uma imagem a partir do array numpy.\n",
        "\n",
        "    Args:\n",
        "        row_index (int): Índice da linha da imagem a ser reconstruída.\n",
        "        image_array (np.ndarray): Array numpy contendo as imagens.\n",
        "    \"\"\"\n",
        "    image_array = image_array[row_index]\n",
        "    image_reshaped = np.reshape(image_array, (10, 12))\n",
        "\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.imshow(image_reshaped, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = ler_arq_imagens('X.txt')\n",
        "reconstruct_image(5,image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Função para matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "\n",
        "def plotar_matriz_de_confusao(predictions, y_test):\n",
        "    \"\"\"\n",
        "    Traduz os dados dos arrays predictions e y_test para outros dois arrays\n",
        "    apenas contendo as letras previstas e originais de teste e plota uma matriz\n",
        "    de confusão usando o sklearn.metrics\n",
        "\n",
        "    Args:\n",
        "        predictions: Array contendo os resultados previstos pelo MLP.\n",
        "        y_test: Array contendo os dados de teste do MLP\n",
        "    \"\"\"\n",
        "    \n",
        "    # Listas que irão conter as letras previstas e de teste\n",
        "    predictions_letras = []\n",
        "    y_test_letras = []\n",
        "\n",
        "    # Converter os arrays de predictions e y_test para letras\n",
        "    i = 0\n",
        "    for i in range(len(predictions)):\n",
        "        letra = vetor_para_letra(predictions[i])\n",
        "        predictions_letras.append(letra)\n",
        "    \n",
        "    j = 0\n",
        "    for j in range(len(predictions)):    \n",
        "       letra = vetor_para_letra(y_test[j])\n",
        "       y_test_letras.append(letra)\n",
        "\n",
        "    # Array com todas as possibilidades, ou seja, o alfabeto de 'A' a 'Z'\n",
        "    classes = [chr(i) for i in range(65, 91)] \n",
        "        \n",
        "    cm = confusion_matrix(y_test_letras, predictions_letras, labels= classes)\n",
        "\n",
        "    # Plot usando ConfusionMatrixDisplay\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix using ConfusionMatrixDisplay')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot usando seaborn\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix using Seaborn')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funções de ativação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LDKiNpRBh7Cd"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x, der=False):\n",
        "    if der:\n",
        "        fx = sigmoid(x)\n",
        "        return fx * (1 - fx)\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x, der=False):\n",
        "    if der:\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x, der=False):\n",
        "    if der:\n",
        "        return np.where(x > 0, 1, 0)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x, der=False):\n",
        "    alpha = 0.01\n",
        "    if der:\n",
        "        return np.where(x > 0, 1, alpha)\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def soft_max(x, der=False):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "def activation_function(x, func, der=False):\n",
        "    return func(x, der)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Função para inicialização de pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_xavier(n_in, n_out):\n",
        "    \"\"\"\n",
        "    Inicializa os pesos com a inicialização Xavier.\n",
        "\n",
        "    Args:\n",
        "        n_in (int): Número de unidades na camada de entrada (ou unidades na camada anterior).\n",
        "        n_out (int): Número de unidades na camada de saída.\n",
        "\n",
        "    Returns:\n",
        "        (numpy.ndarray): Uma matriz de pesos inicializados.\n",
        "    \"\"\"\n",
        "    stddev = np.sqrt(2.0 / (n_in + n_out))\n",
        "    return np.random.randn(n_in, n_out) * stddev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classes da MLP (e início efetivo da codificação da rede neural)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XJnD2TDHZgUw"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, learning_rate, act_fun, output_layer=False):\n",
        "        \"\"\"\n",
        "        Inicializa uma nova camada na rede neural.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Número de neurônios na camada anterior ou tamanho dos dados de entrada.\n",
        "            output_size (int): Número de neurônios na camada atual.\n",
        "            learning_rate (float): Taxa de aprendizado usada para ajustar os pesos e biases durante o treinamento.\n",
        "            act_fun (function): Função de ativação da camada.\n",
        "            output_layer (bool): Booleano que indica se a camada é a de saída da MLP ou não.\n",
        "\n",
        "        Attributes:\n",
        "            weights (np.array): Matriz de pesos, onde cada peso conecta um neurônio de entrada a um neurônio de saída.\n",
        "            biases (np.array): Vetor de biases, um para cada neurônio de saída.\n",
        "            weighted_input (np.array): Armazena a entrada ponderada (antes da aplicação de qualquer função de ativação).\n",
        "            output_data (np.array): Armazena a saída da camada, que neste caso é simplesmente a entrada ponderada.\n",
        "            input_data (np.array): Armazena a entrada da camada antes da ponderação.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.act_fun = act_fun\n",
        "        self.output_layer = output_layer\n",
        "        self.weights = init_xavier(input_size, output_size) \n",
        "        self.biases = np.random.randn(1, output_size) \n",
        "        self.weighted_input = None\n",
        "        self.output_data = None\n",
        "        self.input_data = None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        \"\"\"\n",
        "        Realiza o feed forward da camada.\n",
        "\n",
        "        :param input_data: Vetor de dados de entrada.\n",
        "        :return: Saída após multiplicação pelos pesos e função de ativação.\n",
        "        \"\"\"\n",
        "        self.input_data = input_data\n",
        "        self.weighted_input = np.dot(input_data, self.weights) + self.biases\n",
        "        self.output_data = activation_function(self.weighted_input, self.act_fun)\n",
        "\n",
        "        return self.output_data\n",
        "\n",
        "    def backward(self, error):\n",
        "        \"\"\"\n",
        "        Realiza o backward propagation da camada.\n",
        "\n",
        "        :param error: Erros que serão usados para calcular a correção dos pesos.\n",
        "        :return: Gradientes para serem usados na correção da camada abaixo.\n",
        "        \"\"\"\n",
        "        # Caso não seja a camada de saída, multiplica o erro pela derivada da função de ativação sobre a entrada ponderada da camada.\n",
        "        if not self.output_layer:\n",
        "            error = error * activation_function(self.weighted_input, self.act_fun, True)\n",
        "\n",
        "        if self.input_data.ndim == 1:\n",
        "            self.input_data = self.input_data.reshape(1, -1)  # Garante que input_data é bidimensional\n",
        "        if error.ndim == 1:\n",
        "            error = error.reshape(1, -1)  # Garante que error é bidimensional\n",
        "\n",
        "        # Calcula correções e gradientes\n",
        "        input_error = np.dot(error, self.weights.T)\n",
        "        weights_error = np.dot(self.input_data.T, error) \n",
        "\n",
        "        # Atualiza pesos e biases\n",
        "        self.weights += self.learning_rate * weights_error\n",
        "        self.biases += self.learning_rate * np.sum(error, axis=0, keepdims=True)\n",
        "        return input_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VQ_7l4FUsnEh"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Inicializa a rede neural com camadas especificadas.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float): Taxa de aprendizado usada para ajustar os pesos e biases durante o treinamento.\n",
        "\n",
        "        Attributes:\n",
        "            learning_rate (float): Taxa de aprendizado da rede neural.\n",
        "            hidden_layer1 (Layer): Primeira camada oculta da rede neural.\n",
        "            output_layer (Layer): Camada de saída da rede neural.\n",
        "            final_output (np.array): Armazena a saída final da rede após a propagação direta.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.hidden_layer1 = Layer(120, 120, self.learning_rate, sigmoid)\n",
        "        self.output_layer = Layer(120, 26, self.learning_rate, leaky_relu, True)\n",
        "        self.final_output = None\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        \"\"\"\n",
        "        Realiza a propagação para frente através de toda a rede.\n",
        "\n",
        "        Args:\n",
        "            input_data (np.array): Dados de entrada para a rede.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Saída final da rede.\n",
        "        \"\"\"\n",
        "        output = self.hidden_layer1.forward(input_data)\n",
        "        self.final_output = self.output_layer.forward(output)\n",
        "        return self.final_output\n",
        "\n",
        "    def back_propagation(self, output_error):\n",
        "        \"\"\"\n",
        "        Realiza a propagação para trás através de toda a rede.\n",
        "\n",
        "        Args:\n",
        "            output_error (np.array): Erro na saída da rede.\n",
        "        \"\"\"\n",
        "        error = self.output_layer.backward(output_error)\n",
        "        self.hidden_layer1.backward(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultilayerPerceptron:\n",
        "    def __init__(self, early_stop_param=0, learning_rate=0, min_training_error=0, max_epochs_num=0):\n",
        "        \"\"\"\n",
        "        Inicializa os parâmetros para o treinamento da rede neural.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float): Taxa de aprendizado do modelo.\n",
        "            early_stop_param (int): Número de épocas sem diminuição de erro permitido pelo earlystop.\n",
        "            min_training_error_error (float): Erro mínimo desejado para o treinamento.\n",
        "            max_epochs_num (int): Número máximo de épocas para o treinamento.\n",
        "\n",
        "        Attributes:\n",
        "            learning_rate (float): Taxa de aprendizado do modelo.\n",
        "            early_stop_param (int): Número de épocas sem diminuição de erro permitido pelo earlystop.\n",
        "            min_training_error_error (float): Erro mínimo desejado para o treinamento.\n",
        "            max_epochs_num (int): Número máximo de épocas para o treinamento.\n",
        "            min_error (float): Menor erro de validação encontrado.\n",
        "            negative_error_var_num (int): Contador de épocas com aumento do erro.\n",
        "            current_epoch_error (float): Taxa de erro da época atual.\n",
        "            last_epoch_error (float): Taxa de erro da última época.\n",
        "            neural_network (NeuralNetwork): Rede neural do modelo.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.early_stop_param = early_stop_param\n",
        "        self.min_training_error = min_training_error\n",
        "        self.max_epochs_num = max_epochs_num\n",
        "        self.min_error = 1\n",
        "        self.negative_error_var_num = 0\n",
        "        self.current_epoch_error = 0\n",
        "        self.last_epoch_error = 0\n",
        "\n",
        "        # Lista para armazenar os erros de treinamento em cada época\n",
        "        self.training_errors = []\n",
        "\n",
        "        # Lista para armazenar os erros de validação em cada época (em caso de earlystop)\n",
        "        self.validation_errors = []\n",
        "\n",
        "        # Inicializa a rede neural\n",
        "        self.neural_network = NeuralNetwork(learning_rate=0.01)\n",
        "        \n",
        "    def train(self, entrada, classes, earlystop=False):\n",
        "        \"\"\"\n",
        "        Treina a rede neural usando o conjunto de dados de treinamento.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Dados de entrada para treinamento.\n",
        "            classes (np.array): Labels correspondentes para o treinamento.\n",
        "            earlystop (boolean): Booleano que indica se earlystop será usado ou não\n",
        "        \"\"\"\n",
        "        # Treina a rede com earlystop caso o parâmetro seja verdadeiro\n",
        "        if earlystop: \n",
        "\n",
        "            # Divide os dados de entrada num conjunto de treinamento e outro de validação\n",
        "            x_train, y_train, x_val, y_val = dividir_dados(entrada, classes)\n",
        "\n",
        "            # Cria uma variável que irá armazenar a rede neural com o menor erro encontrado\n",
        "            best_neural_network = copy.deepcopy(self.neural_network)\n",
        "            \n",
        "            # Treina a MLP\n",
        "            for epoch in range(self.max_epochs_num):\n",
        "                epoch_error = 0\n",
        "\n",
        "                # Loop que treina a MLP com os dados de treinamento\n",
        "                for x, y in zip(x_train, y_train):\n",
        "                    # Realiza o feed forward\n",
        "                    output = self.neural_network.forward_propagation(x)\n",
        "\n",
        "                    # Calcula o erro de saída\n",
        "                    output_error = y - output\n",
        "\n",
        "                    # Realiza o back propagation\n",
        "                    self.neural_network.back_propagation(output_error)\n",
        "\n",
        "                    # Calcula o erro quadrático médio e soma ao erro médio da época\n",
        "                    epoch_error += np.mean((np.power(output_error, 2))/2)\n",
        "\n",
        "                # Armazena erro de treinamento médio\n",
        "                train_error = epoch_error / x_train.shape[0]\n",
        "\n",
        "                # Calcula a acurácia e erro de validação do modelo\n",
        "                acuracia, val_error = self.get_accuracy(x_val, y_val)\n",
        "                \n",
        "                # Armazena os erros nos vetores de erros\n",
        "                self.training_errors.append(train_error)\n",
        "                self.validation_errors.append(val_error)\n",
        "\n",
        "                # Printa erro e acurácia a cada dez épocas\n",
        "                if epoch % 10 == 0:\n",
        "                    print(\n",
        "                        f\"Época {epoch}/{self.max_epochs_num}, Acurácia: {acuracia:.5f}, Erro de validação: {val_error:.5f}, Erro de treino: {train_error:.5f}\")\n",
        "                \n",
        "                # Verifica parâmetros de earlystop\n",
        "                if train_error < self.min_training_error:\n",
        "                    print(\"Erro mínimo atingido. Parando o treinamento.\")\n",
        "                    break\n",
        "                \n",
        "                # Verifica se foi encontrado um erro menor que o mínimo registrado\n",
        "                if val_error <= self.min_error:\n",
        "\n",
        "                    # Atualiza menor erro encontrado\n",
        "                    self.min_error = val_error\n",
        "\n",
        "                    # Zera o contador de aumento de erro\n",
        "                    self.negative_error_var_num = 0\n",
        "\n",
        "                    # Salva a rede neural que obteve o menor erro\n",
        "                    best_neural_network = copy.deepcopy(self.neural_network)\n",
        "\n",
        "                # Se não, aumenta o contador de épocas sem diminuição do erro\n",
        "                else:\n",
        "                    self.negative_error_var_num += 1\n",
        "\n",
        "                if self.negative_error_var_num >= self.early_stop_param:\n",
        "                    print(\"Parando antecipadamente devido ao aumento do erro de validação.\")\n",
        "                    # Restaura o melhor conjunto de pesos encontrado\n",
        "                    self.neural_network = copy.deepcopy(best_neural_network)\n",
        "                    break\n",
        "        \n",
        "        # Treina a rede sem earlystop\n",
        "        else:\n",
        "            # Treina a MLP\n",
        "            for epoch in range(self.max_epochs_num):\n",
        "                epoch_error = 0\n",
        "\n",
        "                # Loop que treina a MLP com os dados passados como parâmetro\n",
        "                for x, y in zip(entrada, classes):\n",
        "                    # Realiza o feed forward\n",
        "                    output = self.neural_network.forward_propagation(x)\n",
        "\n",
        "                    # Calcula o erro de saída\n",
        "                    output_error = y - output\n",
        "\n",
        "                    # Realiza o back propagation\n",
        "                    self.neural_network.back_propagation(output_error)\n",
        "\n",
        "                    # Calcula o erro quadrático médio e soma ao erro médio da época\n",
        "                    epoch_error += np.mean((np.power(output_error, 2))/2)\n",
        "\n",
        "                # Armazena erro de treinamento médio\n",
        "                train_error = epoch_error / entrada.shape[0]\n",
        "                \n",
        "                # Armazena os erros nos vetores de erros\n",
        "                self.training_errors.append(train_error)\n",
        "\n",
        "                # Printa erro e acurácia a cada dez épocas\n",
        "                if epoch % 10 == 0:\n",
        "                    print(\n",
        "                        f\"Época {epoch}/{self.max_epochs_num}, Erro de treino: {train_error:.5f}\")\n",
        "\n",
        "    def get_accuracy(self, entrada, saida_esperada):\n",
        "        \"\"\"\n",
        "        Obtém a acurácia e erro médio da MLP.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Dados de entrada para teste.\n",
        "            saida_esperada (np.array): Saída esperada da rede neural.\n",
        "\n",
        "        Returns:\n",
        "            acuracia (float): Porcentagem de acertos da rede.\n",
        "            erro_medio (float): Erro médio \n",
        "        \"\"\"\n",
        "\n",
        "        # Inicializa variáveis para calcular acurácia\n",
        "        num_testes = 0\n",
        "        num_acertos = 0\n",
        "        erro = 0\n",
        "\n",
        "        # Realiza teste para cada um dos dados de entrada\n",
        "        for x, y in zip(entrada, saida_esperada):\n",
        "            num_testes += 1\n",
        "            \n",
        "            # Realiza a predição\n",
        "            predicao = self.neural_network.forward_propagation(x)\n",
        "\n",
        "            # Calcula erro\n",
        "            output_error = y - predicao\n",
        "            erro += np.mean((np.power(output_error, 2) / 2))\n",
        "\n",
        "            # Obtém a letra predita pelo modelo\n",
        "            letra_predita = vetor_para_letra(predicao)\n",
        "\n",
        "            # Obtém a letra esperada\n",
        "            letra_real = vetor_para_letra(y)\n",
        "\n",
        "            if letra_predita == letra_real:\n",
        "                num_acertos += 1\n",
        "\n",
        "        \n",
        "        acuracia = num_acertos / num_testes\n",
        "        erro_medio = erro / entrada.shape[0]\n",
        "\n",
        "        return acuracia, erro_medio\n",
        "\n",
        "    def predict(self, entrada):\n",
        "        \"\"\"\n",
        "        Realiza predições usando a rede neural treinada.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Dados de entrada para teste.\n",
        "            neural_network (NeuralNetwork): Instância da rede neural treinada.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Predições da rede neural.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x in entrada:\n",
        "            output = self.neural_network.forward_propagation(x)\n",
        "            predictions.append(output)\n",
        "        return np.array(predictions)\n",
        "    \n",
        "    def save_mlp(self, nome_arq):\n",
        "        \"\"\"\n",
        "        Armazena os pesos das camadas e os erros médios de cada época num arquivo.\n",
        "\n",
        "        Args:\n",
        "            nome_arq (string): Nome do arquivo que irá armazenar os valores.\n",
        "        \"\"\" \n",
        "\n",
        "        network = { \n",
        "            # Conversão dos pesos da primeira camada oculta para uma lista\n",
        "            'hidden_layer1': {'weights': self.neural_network.hidden_layer1.weights.tolist(),\n",
        "                            'biases': self.neural_network.hidden_layer1.biases.tolist()},\n",
        "\n",
        "            # Conversão dos pesos da camada de saída para uma lista\n",
        "            'output_layer': {'weights': self.neural_network.output_layer.weights.tolist(),\n",
        "                            'biases': self.neural_network.output_layer.biases.tolist()},\n",
        "            \n",
        "            'train_errors' : self.training_errors,\n",
        "            'val_errors' : self.validation_errors\n",
        "        }\n",
        "\n",
        "        # Abertura do arquivo para escrita\n",
        "        with open(nome_arq, 'w') as f:\n",
        "            # Salvando o dicionário no formato JSON\n",
        "            json.dump(network, f) \n",
        "\n",
        "    def build_mlp(self, nome_arq):\n",
        "        \"\"\"\n",
        "        Constrói a MLP a partir de um arquivo de pesos.\n",
        "\n",
        "        Args:\n",
        "            nome_arq (string): Nome do arquivo com os pesos e biases de cada camada.\n",
        "        \"\"\"\n",
        "        with open(nome_arq, 'r') as f:\n",
        "            network = json.load(f)\n",
        "\n",
        "        self.neural_network.hidden_layer1.weights = np.array(network['hidden_layer1']['weights'])\n",
        "        self.neural_network.hidden_layer1.biases = np.array(network['hidden_layer1']['biases'])\n",
        "        self.neural_network.output_layer.weights = np.array(network['output_layer']['weights'])\n",
        "        self.neural_network.output_layer.biases = np.array(network['output_layer']['biases'])\n",
        "\n",
        "        self.training_errors = network['train_errors']\n",
        "        self.validation_errors = network['val_errors']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lista de parâmetros para testar\n",
        "\n",
        "MLP PADRÃO:\n",
        "- Mostrar funcionamento da MLP e divisão do conjunto de dados\n",
        "    - Mostra o que é exibido no terminal durante o treinamento \n",
        "        - Erro a cada 10 épocas\n",
        "        - Erro médio e acurácia do modelo ao final do treinamento\n",
        "    - Motra que os dados foram dividios na proporção 80/20, aleatoriamente, e com a distribuição uniforme de classes\n",
        "\n",
        "- Teste de funções de ativação diferentes\n",
        "    - Exibe gráfico de erro para cada função de ativação diferente\n",
        "    - Acurácia final do modelo para cada função de ativação diferente\n",
        "- Teste de learning rate, números de neruônios diferentes e inicialização com Xavier\n",
        "    - Exibe gráfico de erro para cada learning rate e número de neurônios diferentes\n",
        "    - Exibe acurácia para cada teste diferente\n",
        "    - Após definidos learning rate e número de neurônios, exibe gráfico de erro e acurácia antes e depois do Xavier\n",
        "E\n",
        " YLR\n",
        "- Teste earlystop\n",
        "- Teste cross validationer- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main: treina o modelo, printa a execução de cada época e depois obtém a acurácia e erro médio final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Época 0/500, Acurácia: 0.03846, Erro de validação: 0.07430, Erro de treino: 0.09921\n",
            "Época 10/500, Acurácia: 0.28462, Erro de validação: 0.07008, Erro de treino: 0.07068\n",
            "Época 20/500, Acurácia: 0.73846, Erro de validação: 0.06137, Erro de treino: 0.06133\n",
            "Época 30/500, Acurácia: 0.83462, Erro de validação: 0.05267, Erro de treino: 0.05154\n",
            "Época 40/500, Acurácia: 0.86538, Erro de validação: 0.04635, Erro de treino: 0.04368\n",
            "Época 50/500, Acurácia: 0.88077, Erro de validação: 0.04514, Erro de treino: 0.03814\n",
            "Época 60/500, Acurácia: 0.87308, Erro de validação: 0.04802, Erro de treino: 0.03507\n",
            "Época 70/500, Acurácia: 0.87692, Erro de validação: 0.04835, Erro de treino: 0.03203\n",
            "Época 80/500, Acurácia: 0.87692, Erro de validação: 0.05883, Erro de treino: 0.03079\n",
            "Época 90/500, Acurácia: 0.87692, Erro de validação: 0.05920, Erro de treino: 0.03030\n",
            "Parando antecipadamente devido ao aumento do erro de validação.\n",
            "Treinamento concluído.\n",
            "Acurácia do modelo: 0.8846153846153846\n",
            "Erro médio do modelo: 0.04531975486867748\n",
            "Predição: D, Real: D\n",
            "Predição: C, Real: C\n",
            "Predição: B, Real: R\n",
            "Predição: B, Real: B\n",
            "Predição: L, Real: L\n",
            "Predição: B, Real: B\n",
            "Predição: P, Real: P\n",
            "Predição: Z, Real: Z\n",
            "Predição: S, Real: S\n",
            "Predição: G, Real: G\n",
            "Predição: A, Real: A\n",
            "Predição: E, Real: E\n",
            "Predição: V, Real: V\n",
            "Predição: O, Real: O\n",
            "Predição: V, Real: V\n",
            "Predição: B, Real: B\n",
            "Predição: X, Real: X\n",
            "Predição: F, Real: F\n",
            "Predição: F, Real: F\n",
            "Predição: N, Real: N\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Lendo os arquivos\n",
        "    imagens = ler_arq_imagens('X.txt')\n",
        "    classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "    # Dividindo os dados em conjuntos de treinamento e teste\n",
        "    x_train, y_train, x_test, y_test = dividir_dados(imagens, classes, test_proportion=0.05)\n",
        "\n",
        "    # Convertendo para float\n",
        "    y_train = y_train.astype(float)\n",
        "    y_test = y_test.astype(float)\n",
        "\n",
        "    # Parâmetros para o treinamento\n",
        "    early_stop_param = 50\n",
        "    learning_rate = 0.1\n",
        "    min_output_error = 0.01\n",
        "    max_epochs_num = 500\n",
        "\n",
        "    # Inicializa o Multilayer Perceptron com os parâmetros de treinamento\n",
        "    mlp = MultilayerPerceptron(early_stop_param, learning_rate, min_output_error, max_epochs_num)\n",
        "\n",
        "    # Treina a rede neural\n",
        "    mlp.train(x_train, y_train, earlystop=True)\n",
        "\n",
        "    print(\"Treinamento concluído.\")\n",
        "\n",
        "    acuracia, erro = mlp.get_accuracy(x_test, y_test)\n",
        "    print(f\"Acurácia do modelo: {acuracia}\")\n",
        "    print(f\"Erro médio do modelo: {erro}\")\n",
        "\n",
        "    # Testa a rede neural\n",
        "    predictions = mlp.predict(x_test)\n",
        "\n",
        "    # Exibe algumas predições para verificação\n",
        "    for i in range(20):\n",
        "        predicao = vetor_para_letra(predictions[i])\n",
        "        correto = vetor_para_letra(y_test[i])\n",
        "\n",
        "        print(f\"Predição: {predicao}, Real: {correto}\")\n",
        "\n",
        "    # Armazena MLP num arquivo\n",
        "    mlp.save_mlp(\"mlp.txt\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Teste e exemplo da função de reconstruir MLP a partir do arquivo de pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def teste_build_mlp():\n",
        "    # Lendo os arquivos\n",
        "    imagens = ler_arq_imagens('X.txt')\n",
        "    classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "    # Dividindo os dados em conjuntos de treinamento e teste\n",
        "    x_train, y_train, x_test, y_test = dividir_dados(imagens, classes, test_proportion=0.5)\n",
        "\n",
        "    # Inicializa MLP sem nenhum parâmetro \n",
        "    mlp = MultilayerPerceptron()\n",
        "\n",
        "    # Chama a função para construir os pesos a partir do arquivo\n",
        "    mlp.build_mlp('mlp.txt')\n",
        "\n",
        "    # Testa a rede neural\n",
        "    predictions = mlp.predict(x_test)\n",
        "\n",
        "    acuracia, erro = mlp.get_accuracy(x_test, y_test)\n",
        "    print(f\"Acurácia do modelo: {acuracia}\")\n",
        "    print(f\"Erro médio do modelo: {erro}\")\n",
        "\n",
        "    # Exibe algumas predições para verificação\n",
        "    for i in range(20):\n",
        "        predicao = vetor_para_letra(predictions[i])\n",
        "        correto = vetor_para_letra(y_test[i])\n",
        "\n",
        "        print(f\"Predição: {predicao}, Real: {correto}\")\n",
        "    \n",
        "    print(mlp.training_errors)\n",
        "    print(mlp.validation_errors)\n",
        "\n",
        "    plotar_matriz_de_confusao(predictions, y_test)\n",
        "\n",
        "teste_build_mlp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kfold(entrada, classes, num_particoes):\n",
        "    \"\"\"\n",
        "    Divide os dados e classes em K partições para K-Fold Cross-Validation.\n",
        "\n",
        "    Args:\n",
        "        entrada (np.array): Vetor dos dados a serem divididos.\n",
        "        classes (np.array): Vetor das classes dos dados.\n",
        "        num_particoes (int): Número de partições (K).\n",
        "\n",
        "    Returns:\n",
        "        dados_particionados (list): Lista de K partições dos dados.\n",
        "        classes_particionadas (list): Lista de K partições das classes.\n",
        "    \"\"\"\n",
        "    # Transforma os vetores das classes nas letras correspondentes\n",
        "    classes_letras = []\n",
        "    for elemento in classes:\n",
        "        classes_letras.append(vetor_para_letra(elemento))\n",
        "\n",
        "    classes_letras = np.array(classes_letras)\n",
        "\n",
        "    # Conta o número de classes\n",
        "    unique_classes, class_counts = np.unique(classes_letras, return_counts=True)\n",
        "    \n",
        "    # Calcula a quantidade desejada de cada classe no primeiro conjunto\n",
        "    desired_counts = (class_counts * (1 - test_proportion)).astype(int)\n",
        "\n",
        "    # Define o tamanho de cada partição\n",
        "    tam_entrada = entrada.shape[0]\n",
        "    tam_particoes = int(tam_entrada / num_particoes)\n",
        "    particoes = []\n",
        "\n",
        "\n",
        "    # Divide o vetor de índice em K partes\n",
        "    for i in range(num_particoes):\n",
        "        if i != num_particoes - 1:\n",
        "            particoes.append(indices[(i * tam_particoes) : ((i + 1) * tam_particoes)])\n",
        "        else:\n",
        "            particoes.append(indices[(i * tam_particoes) :])\n",
        "\n",
        "    dados_particionados = []\n",
        "    classes_particionadas = []\n",
        "\n",
        "    # Define os vetores de dados e classificação de cada partição\n",
        "    for p in particoes:\n",
        "        dados_particionados.append(entrada[p])\n",
        "        classes_particionadas.append(classes[p])\n",
        "\n",
        "    return dados_particionados, classes_particionadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "        Divide dois vetores de dados em uma determinada proporção (padrão: 80/20).\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Vetor dos dados a serem divididos.\n",
        "            classes (np.array): Classificação dos dados a serem dividos.\n",
        "            div_proportion (float): Proporção da divisão dos dados (padrão: 20%)\n",
        "\n",
        "        Returns:\n",
        "            dados_primeira_parte (np.array): Vetor de dados da primeira parte da divisão.\n",
        "            classes_primeira_parte (np.array): Vetor de classificação dos dados da primeira parte da divisão.\n",
        "            dados_segunda_parte (np.array): Vetor de dados da segunda parte da divisão.\n",
        "            classes_segunda_parte (np.array): Vetor de classificação dos dados da segunda parte da divisão.\n",
        "        \"\"\"\n",
        "    # Transforma os vetores das classes nas letras correspondentes\n",
        "    classes_letras = []\n",
        "    for elemento in classes:\n",
        "        classes_letras.append(vetor_para_letra(elemento))\n",
        "\n",
        "    classes_letras = np.array(classes_letras)\n",
        "\n",
        "    # Conta o número de classes\n",
        "    unique_classes, class_counts = np.unique(classes_letras, return_counts=True)\n",
        "    \n",
        "    # Calcula a quantidade desejada de cada classe no primeiro conjunto\n",
        "    desired_counts = (class_counts * (1 - test_proportion)).astype(int)\n",
        "    \n",
        "    # Inicializa os vetores para as duas partes\n",
        "    dados_primeira_parte = []\n",
        "    classes_primeira_parte = []\n",
        "    dados_segunda_parte = []\n",
        "    classes_segunda_parte = []\n",
        "    \n",
        "    for classe, desired_count in zip(unique_classes, desired_counts):\n",
        "        # Índices das amostras da classe atual\n",
        "        indices_classe = np.where(classes_letras == classe)[0]\n",
        "\n",
        "        # Seleciona aleatoriamente as amostras para o primeiro conjunto\n",
        "        selected_indices = np.random.choice(indices_classe, size=desired_count, replace=False)\n",
        "        \n",
        "        # Divide os dados e classes\n",
        "        dados_primeira_parte.append(entrada[selected_indices])\n",
        "        classes_primeira_parte.append(classes[selected_indices])\n",
        "        \n",
        "        # As amostras restantes vão para o segundo conjunto\n",
        "        remaining_indices = np.setdiff1d(indices_classe, selected_indices)\n",
        "        dados_segunda_parte.append(entrada[remaining_indices])\n",
        "        classes_segunda_parte.append(classes[remaining_indices])\n",
        "    \n",
        "    # Concatena os vetores\n",
        "    dados_primeira_parte = np.concatenate(dados_primeira_parte)\n",
        "    classes_primeira_parte = np.concatenate(classes_primeira_parte)\n",
        "    dados_segunda_parte = np.concatenate(dados_segunda_parte)\n",
        "    classes_segunda_parte = np.concatenate(classes_segunda_parte)\n",
        "\n",
        "    dados_primeira_parte, classes_primeira_parte = shuffle_dados(dados_primeira_parte, classes_primeira_parte)\n",
        "    dados_segunda_parte, classes_segunda_parte = shuffle_dados(dados_segunda_parte, classes_segunda_parte)\n",
        "\n",
        "    return dados_primeira_parte, classes_primeira_parte, dados_segunda_parte, classes_segunda_parte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1326, 120)\n",
            "(1326, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(266, 120)\n",
            "(266, 26)\n"
          ]
        }
      ],
      "source": [
        "# Teste função K-Fold\n",
        "imagens = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "print(imagens.shape)\n",
        "print(classes.shape)\n",
        "\n",
        "imagens_part, classes_part = kfold(imagens, classes, 5)\n",
        "\n",
        "for i in range(5):\n",
        "    print(imagens_part[i].shape)\n",
        "    print(classes_part[i].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinamento K-Fold 1: \n",
            "Época 0/300, Acurácia: 0.06787330316742081, Erro de validação: 0.1503282946680903, Erro de treino: 0.2074031354575942\n",
            "Época 10/300, Acurácia: 0.09502262443438914, Erro de validação: 0.1427572774062789, Erro de treino: 0.1422337745186372\n",
            "Época 20/300, Acurácia: 0.5565610859728507, Erro de validação: 0.12972794325928058, Erro de treino: 0.12842902961446515\n",
            "Época 30/300, Acurácia: 0.7013574660633484, Erro de validação: 0.11663740054126306, Erro de treino: 0.11287732914744321\n",
            "Época 40/300, Acurácia: 0.8099547511312217, Erro de validação: 0.10659234912240378, Erro de treino: 0.09877656977632013\n",
            "Época 50/300, Acurácia: 0.7918552036199095, Erro de validação: 0.10033463171428622, Erro de treino: 0.08753008503108323\n",
            "Época 60/300, Acurácia: 0.8144796380090498, Erro de validação: 0.0957137401220487, Erro de treino: 0.08017770622659245\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Convertendo para float\u001b[39;00m\n\u001b[0;32m     46\u001b[0m classes \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[43mtrain_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[42], line 30\u001b[0m, in \u001b[0;36mtrain_cross_validation\u001b[1;34m(entrada, classes, num_particoes)\u001b[0m\n\u001b[0;32m     27\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MultilayerPerceptron(min_output_error, max_epochs_num, early_stop_param, learning_rate)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Treina MLP\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdados_treinamento\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_treinamento\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinamento \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m finalizado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m acuracia \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mget_accuracy(dados_teste, classes_teste)\n",
            "Cell \u001b[1;32mIn[38], line 57\u001b[0m, in \u001b[0;36mMultilayerPerceptron.train\u001b[1;34m(self, entrada, classes, earlystop)\u001b[0m\n\u001b[0;32m     55\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneural_network\u001b[38;5;241m.\u001b[39mforward_propagation(x)\n\u001b[0;32m     56\u001b[0m     output_error \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m output\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     epoch_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mpower(output_error, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     60\u001b[0m train_error \u001b[38;5;241m=\u001b[39m epoch_error \u001b[38;5;241m/\u001b[39m x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
            "Cell \u001b[1;32mIn[12], line 42\u001b[0m, in \u001b[0;36mNeuralNetwork.back_propagation\u001b[1;34m(self, output_error)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03mRealiza a propagação para trás através de toda a rede.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    output_error (np.array): Erro na saída da rede.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39mbackward(output_error)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[32], line 61\u001b[0m, in \u001b[0;36mLayer.backward\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m     58\u001b[0m     error \u001b[38;5;241m=\u001b[39m error\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Garante que error é bidimensional\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Calcula correções e gradientes\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m input_error \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m weights_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data\u001b[38;5;241m.\u001b[39mT, error) \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Atualiza pesos e biases\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_cross_validation(entrada, classes, num_particoes):\n",
        "    \"\"\"\n",
        "    Realiza o treinamento com cross validation de uma MLP.\n",
        "\n",
        "    Args:\n",
        "        entrada (np.array): Vetor dos dados de entrada.\n",
        "        classes (np.array): Vetor das classes dos dados.\n",
        "        num_particoes (int): Número de partições (K).\n",
        "    \"\"\"\n",
        "    particoes_dados, particoes_classes = kfold(entrada, classes, num_particoes)\n",
        "\n",
        "    min_output_error = 0.01\n",
        "    max_epochs_num = 300\n",
        "    early_stop_param = 15\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    acuracia_total = 0\n",
        "\n",
        "    for i in range(num_particoes):\n",
        "        print(f\"Treinamento K-Fold {i + 1}: \")\n",
        "\n",
        "        # Define conjunto de testes\n",
        "        dados_teste = particoes_dados[i]\n",
        "        classes_teste = particoes_classes[i]\n",
        "\n",
        "        # Remove conjunto de testes das partições\n",
        "        min_output_error, max_epochs_num, early_stop_param, learning_rate\n",
        "        dados_treinamento = [particao for j, particao in enumerate(particoes_dados) if j != i]\n",
        "        dados_treinamento = np.concatenate(dados_treinamento)\n",
        "        classes_treinamento = [particao for j, particao in enumerate(particoes_classes) if j != i]\n",
        "        classes_treinamento = np.concatenate(classes_treinamento)\n",
        "\n",
        "        # Inicia MLP\n",
        "        mlp = MultilayerPerceptron(min_output_error, max_epochs_num, early_stop_param, learning_rate)\n",
        "\n",
        "        # Treina MLP\n",
        "        mlp.train(dados_treinamento, classes_treinamento)\n",
        "\n",
        "        print(f\"Treinamento {i + 1} finalizado.\")\n",
        "        acuracia = mlp.get_accuracy(dados_teste, classes_teste)\n",
        "        print(f\"Acurácia: {acuracia:.2f}\\n\")\n",
        "\n",
        "        acuracia_total += acuracia\n",
        "    \n",
        "    print(\"Cross-validation concluído.\")\n",
        "    print(f\"Acurácia média: {acuracia_total / num_particoes:.2f}\")\n",
        "\n",
        "# Lendo os arquivos\n",
        "imagens = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "# Convertendo para float\n",
        "classes = classes.astype(float)\n",
        "\n",
        "train_cross_validation(imagens, classes, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6wKX05Ojl8W"
      },
      "source": [
        "## Exemplos de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhSXY_Wzsrnf"
      },
      "outputs": [],
      "source": [
        "imagens = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes('Y_letra.txt')\n",
        "print(imagens.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classes.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executando o treinamento e teste da rede neural\n",
        "mlp = MultilayerPerceptron()\n",
        "mlp.main(imagens, classes)#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotando o erro de treinamento\n",
        "plt.plot(mlp.training_errors)\n",
        "plt.title('Erro de Treinamento por Época')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Erro')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criação de uma tabela com os erros de treinamento\n",
        "df_erros_treinamento = pd.DataFrame({\n",
        "    'Época': range(len(mlp.training_errors)),\n",
        "    'Erro': mlp.training_errors\n",
        "})\n",
        "\n",
        "# Exibindo a tabela\n",
        "print(df_erros_treinamento)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Falta:\n",
        "\n",
        "- fazer o erro parar de diminuir pouco\n",
        "- tirar sklearn do cross-validation\n",
        "- verificar se o gráfico tá gerando certo\n",
        "- gerar matriz de confusão para testes da rede\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Roteiro para vídeo\n",
        "\n",
        "V1:\n",
        "- Parte de treinamento do código (com e sem validação cruzada e parada antecipada) PARA: conjunto de dados CARACTERES COMPLETO (os outros de teste não entram)\n",
        "- Estudo dos parâmetros (busca por valores adequados – grid) (?)\n",
        "- Teste da MLP para o conjunto de dados CARACTERES COMPLETO\n",
        "\n",
        "V2:\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
