{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGUvj5itk-3m"
      },
      "source": [
        "# Trabalho de IA: MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuOcZ8Nto_bY"
      },
      "source": [
        "Integrantes:\n",
        "- Ana Clara das Neves Barreto - \n",
        "- Eloisa Antero Guisse - 13781924\n",
        "- Marcos Martins de Oliveira Pacheco - 13672602\n",
        "- Jamyle Gonçalves Rodrigues Silva -\n",
        "- Lucca Pinto -\n",
        "- Sarah Klock Mauricio - 13673131\n",
        "\n",
        "Divas pop q nao tem hit solo ha 50 anos:\n",
        "- Lady Gaga\n",
        "- Katy Perry\n",
        "- Demi Lovato\n",
        "- Selena Gomez\n",
        "- Jessie J\n",
        "\n",
        "Tarefas:\n",
        "- Fazer leitura dos dados testes\n",
        "- Fazer o loop das épocas\n",
        "- Conferir o backpropagation\n",
        "- Testar com conj binários\n",
        "\n",
        "Depois:\n",
        "Otimização e análise\n",
        "- Parada antecipada\n",
        "- Rever o doc de especificação do trabalho para montar os gráficos\n",
        "- Começar a guardar os pesos (e perguntar pra prof o porquê)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN8ONKTblF9z"
      },
      "source": [
        "## Desenvolvimento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uRTRIhbRmLbE"
      },
      "outputs": [],
      "source": [
        "# Importações\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import math\n",
        "from graphviz import Digraph\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0sIq_i-l53g"
      },
      "outputs": [],
      "source": [
        "alfabeto = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "def letra_para_vetor(letra):\n",
        "    \"\"\"\n",
        "    Transforma um caractere num vetor de 26 posições, no qual a posição da letra correspondente é 1 e o restante é -1.\n",
        "    :param letra: Letra a ser transformada num vetor\n",
        "    :return: Vetor binário de 26 posições\n",
        "    \"\"\"\n",
        "    vetor = [-1] * 26\n",
        "    pos_letra = alfabeto.index(letra)\n",
        "    vetor[pos_letra] = 1\n",
        "\n",
        "    return vetor\n",
        "\n",
        "def vetor_para_letra(vetor):\n",
        "    \"\"\"\n",
        "    Encontra a letra do alfabeto correspondente a um vetor de 26 posições.\n",
        "    :param vetor: Vetor de 26 posições que representa uma letra do alfabeto.\n",
        "    :return: Letra correspondente.\n",
        "    \"\"\"\n",
        "    # Encontra a posição do maior valor no vetor\n",
        "    maior_pos = np.argmax(vetor)\n",
        "\n",
        "    # Encontra a letra correspondente no alfabeto\n",
        "    letra = alfabeto[maior_pos]\n",
        "\n",
        "    return letra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ler_arq_imagens(nome_arq):\n",
        "    \"\"\"\n",
        "    Lê o arquivo de imagens e retorna um array numpy.\n",
        "\n",
        "    Args:\n",
        "        nome_arq (str): Caminho para o arquivo de imagens.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array numpy contendo as imagens.\n",
        "    \"\"\"\n",
        "    with open(nome_arq, 'r') as file:\n",
        "        data = file.read().strip().split('\\n')\n",
        "    data = [list(map(int, filter(lambda x: x.strip(), line.split(',')))) for line in data if line.strip()]\n",
        "    return np.array(data)\n",
        "\n",
        "def ler_arq_classes_vetores(nome_arq):\n",
        "    \"\"\"\n",
        "    Lê o arquivo de classes e retorna um array numpy.\n",
        "\n",
        "    Args:\n",
        "        nome_arq (str): Caminho para o arquivo de classes.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array numpy contendo as classes.\n",
        "    \"\"\"\n",
        "    with open(nome_arq, 'r') as file:\n",
        "        data = []\n",
        "        for line in file:\n",
        "            vetor_letra = letra_para_vetor(line[0])\n",
        "            data.append(vetor_letra)\n",
        "    return np.array(data)\n",
        "\n",
        "def ler_arq_classes_letras(nome_arq):\n",
        "    with open(nome_arq, 'r') as file:\n",
        "        data = []\n",
        "        for line in file:\n",
        "            data.append(line[0])\n",
        "    return np.array(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dividir_dados(entrada, classes, test_proportion=0.2):\n",
        "    \"\"\"\n",
        "        Divide dois vetores de dados em uma determinada proporção (padrão: 80/20).\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Vetor dos dados a serem divididos.\n",
        "            classes (np.array): Classificação dos dados a serem dividos.\n",
        "            div_proportion (float): Proporção da divisão dos dados (padrão: 20%)\n",
        "\n",
        "        Returns:\n",
        "            dados_primeira_parte (np.array): Vetor de dados da primeira parte da divisão.\n",
        "            classes_primeira_parte (np.array): Vetor de classificação dos dados da primeira parte da divisão.\n",
        "            dados_segunda_parte (np.array): Vetor de dados da segunda parte da divisão.\n",
        "            classes_segunda_parte (np.array): Vetor de classificação dos dados da segunda parte da divisão.\n",
        "        \"\"\"\n",
        "    # Embaralha os índices para os dados serem divididos de forma aleatória\n",
        "    indices = np.arange(entrada.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Obtém o índice da separação dos dados\n",
        "    indice_split = int(len(indices) * (1 - test_proportion))\n",
        "\n",
        "    # Divide o vetor de indíce em duas partes\n",
        "    primeira_parte = indices[:indice_split]\n",
        "    segunda_parte = indices[indice_split:]\n",
        "    \n",
        "    # Define os vetores de cada parte\n",
        "    dados_primeira_parte = entrada[primeira_parte]\n",
        "    classes_primeira_parte = classes[primeira_parte]\n",
        "    dados_segunda_parte = entrada[segunda_parte]\n",
        "    classes_segunda_parte = classes[segunda_parte]\n",
        "\n",
        "    return dados_primeira_parte, classes_primeira_parte, dados_segunda_parte, classes_segunda_parte\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'A': 40, 'B': 40, 'C': 40, 'D': 40, 'E': 40, 'F': 40, 'G': 40, 'H': 40, 'I': 40, 'J': 40, 'K': 40, 'L': 40, 'M': 40, 'N': 40, 'O': 40, 'P': 40, 'Q': 40, 'R': 40, 'S': 40, 'T': 40, 'U': 40, 'V': 40, 'W': 40, 'X': 40, 'Y': 40, 'Z': 40})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def dividir_dados_igualmente(entrada, classes, classes_vetores, test_proportion=0.2):\n",
        "\n",
        "    # Contagem de classes\n",
        "    unique_classes, class_counts = np.unique(classes, return_counts=True)\n",
        "    \n",
        "    # Calcula a quantidade desejada de cada classe no primeiro conjunto\n",
        "    desired_counts = (class_counts * (1 - test_proportion)).astype(int)\n",
        "    \n",
        "    # Inicializa os vetores para as duas partes\n",
        "    dados_primeira_parte = []\n",
        "    classes_primeira_parte = []\n",
        "    dados_segunda_parte = []\n",
        "    classes_segunda_parte = []\n",
        "    \n",
        "    for classe, desired_count in zip(unique_classes, desired_counts):\n",
        "        # Índices das amostras da classe atual\n",
        "        indices_classe = np.where(classes == classe)[0]\n",
        "\n",
        "        # Seleciona aleatoriamente as amostras para o primeiro conjunto\n",
        "        selected_indices = np.random.choice(indices_classe, size=desired_count, replace=False)\n",
        "        \n",
        "        # Divide os dados e classes\n",
        "        dados_primeira_parte.append(entrada[selected_indices])\n",
        "        classes_primeira_parte.append(classes_vetores[selected_indices])\n",
        "        \n",
        "        # As amostras restantes vão para o segundo conjunto\n",
        "        remaining_indices = np.setdiff1d(indices_classe, selected_indices)\n",
        "        dados_segunda_parte.append(entrada[remaining_indices])\n",
        "        classes_segunda_parte.append(classes_vetores[remaining_indices])\n",
        "    \n",
        "    # Concatena os vetores\n",
        "    dados_primeira_parte = np.concatenate(dados_primeira_parte)\n",
        "    classes_primeira_parte = np.concatenate(classes_primeira_parte)\n",
        "    dados_segunda_parte = np.concatenate(dados_segunda_parte)\n",
        "    classes_segunda_parte = np.concatenate(classes_segunda_parte)\n",
        "    \n",
        "    return dados_primeira_parte, classes_primeira_parte, dados_segunda_parte, classes_segunda_parte\n",
        "\n",
        "entrada = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes_letras('Y_letra.txt')\n",
        "classes_vetores = ler_arq_classes_vetores('Y_letra.txt')\n",
        "\n",
        "x_train, y_train, x_teste, y_teste = dividir_dados_igualmente(entrada, classes, classes_vetores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-hoEUDxPfc4"
      },
      "source": [
        "Funcao pra reconstruir imagens a partir do array de array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "nD_uUiPPEYlC",
        "outputId": "a9db9514-13b0-4b09-d43e-b36ca64d539c"
      },
      "outputs": [],
      "source": [
        "def reconstruct_image(row_index, image_array):\n",
        "    \"\"\"\n",
        "    Reconstrói e plota uma imagem a partir do array numpy.\n",
        "\n",
        "    Args:\n",
        "        row_index (int): Índice da linha da imagem a ser reconstruída.\n",
        "        image_array (np.ndarray): Array numpy contendo as imagens.\n",
        "    \"\"\"\n",
        "    image_array = image_array[row_index]\n",
        "    image_reshaped = np.reshape(image_array, (10, 12))\n",
        "\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.imshow(image_reshaped, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image = ler_arq_imagens('X.txt')\n",
        "reconstruct_image(5,image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LDKiNpRBh7Cd"
      },
      "outputs": [],
      "source": [
        "# Funções de ativações para testes\n",
        "\n",
        "def sigmoid(x, der=False):\n",
        "    if der:\n",
        "        fx = sigmoid(x)\n",
        "        return fx * (1 - fx)\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x, der=False):\n",
        "    if der:\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x, der=False):\n",
        "    if der:\n",
        "        return np.where(x > 0, 1, 0)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x, der=False):\n",
        "    alpha = 0.01\n",
        "    if der:\n",
        "        return np.where(x > 0, 1, alpha)\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def soft_max(x, der=False):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "def activation_function(x, func, der=False):\n",
        "    return func(x, der)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XJnD2TDHZgUw"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, learning_rate, act_fun, output_layer=False):\n",
        "        \"\"\"\n",
        "        Inicializa uma nova camada na rede neural.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Número de neurônios na camada anterior ou tamanho dos dados de entrada.\n",
        "            output_size (int): Número de neurônios na camada atual.\n",
        "            learning_rate (float): Taxa de aprendizado usada para ajustar os pesos e biases durante o treinamento.\n",
        "            act_fun (function): Função de ativação da camada.\n",
        "            output_layer (bool): Booleano que indica se a camada é a de saída da MLP ou não.\n",
        "\n",
        "        Attributes:\n",
        "            weights (np.array): Matriz de pesos, onde cada peso conecta um neurônio de entrada a um neurônio de saída.\n",
        "            biases (np.array): Vetor de biases, um para cada neurônio de saída.\n",
        "            weighted_input (np.array): Armazena a entrada ponderada (antes da aplicação de qualquer função de ativação).\n",
        "            output_data (np.array): Armazena a saída da camada, que neste caso é simplesmente a entrada ponderada.\n",
        "            input_data (np.array): Armazena a entrada da camada antes da ponderação.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.act_fun = act_fun\n",
        "        self.output_layer = output_layer\n",
        "        self.weights = np.random.randn(input_size, output_size) \n",
        "        self.biases = np.random.randn(1, output_size) \n",
        "        self.weighted_input = None\n",
        "        self.output_data = None\n",
        "        self.input_data = None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        \"\"\"\n",
        "        Realiza o feed forward da camada.\n",
        "\n",
        "        :param input_data: Vetor de dados de entrada.\n",
        "        :return: Saída após multiplicação pelos pesos e função de ativação.\n",
        "        \"\"\"\n",
        "        self.input_data = input_data\n",
        "        self.weighted_input = np.dot(input_data, self.weights) + self.biases\n",
        "        self.output_data = activation_function(self.weighted_input, self.act_fun)\n",
        "\n",
        "        return self.output_data\n",
        "\n",
        "    def backward(self, error):\n",
        "        \"\"\"\n",
        "        Realiza o backward propagation da camada.\n",
        "\n",
        "        :param error: Erros que serão usados para calcular a correção dos pesos.\n",
        "        :return: Gradientes para serem usados na correção da camada abaixo.\n",
        "        \"\"\"\n",
        "        # Caso não seja a camada de saída, multiplica o erro pela derivada da função de ativação sobre a entrada ponderada da camada.\n",
        "        if not self.output_layer:\n",
        "            error = error * activation_function(self.weighted_input, self.act_fun, True)\n",
        "\n",
        "        if self.input_data.ndim == 1:\n",
        "            self.input_data = self.input_data.reshape(1, -1)  # Garante que input_data é bidimensional\n",
        "        if error.ndim == 1:\n",
        "            error = error.reshape(1, -1)  # Garante que error é bidimensional\n",
        "\n",
        "        # Calcula correções e gradientes\n",
        "        input_error = np.dot(error, self.weights.T)\n",
        "        weights_error = np.dot(self.input_data.T, error) \n",
        "\n",
        "        # Atualiza pesos e biases\n",
        "        self.weights += self.learning_rate * weights_error\n",
        "        self.biases += self.learning_rate * np.sum(error, axis=0, keepdims=True)\n",
        "        return input_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VQ_7l4FUsnEh"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Inicializa a rede neural com camadas especificadas.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float): Taxa de aprendizado usada para ajustar os pesos e biases durante o treinamento.\n",
        "\n",
        "        Attributes:\n",
        "            learning_rate (float): Taxa de aprendizado da rede neural.\n",
        "            hidden_layer1 (Layer): Primeira camada oculta da rede neural.\n",
        "            output_layer (Layer): Camada de saída da rede neural.\n",
        "            final_output (np.array): Armazena a saída final da rede após a propagação direta.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.hidden_layer1 = Layer(120, 120, self.learning_rate, tanh)\n",
        "        self.output_layer = Layer(120, 26, self.learning_rate, leaky_relu, True)\n",
        "        self.final_output = None\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        \"\"\"\n",
        "        Realiza a propagação para frente através de toda a rede.\n",
        "\n",
        "        Args:\n",
        "            input_data (np.array): Dados de entrada para a rede.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Saída final da rede.\n",
        "        \"\"\"\n",
        "        output = self.hidden_layer1.forward(input_data)\n",
        "        self.final_output = self.output_layer.forward(output)\n",
        "        return self.final_output\n",
        "\n",
        "    def back_propagation(self, output_error):\n",
        "        \"\"\"\n",
        "        Realiza a propagação para trás através de toda a rede.\n",
        "\n",
        "        Args:\n",
        "            output_error (np.array): Erro na saída da rede.\n",
        "        \"\"\"\n",
        "        error = self.output_layer.backward(output_error)\n",
        "        self.hidden_layer1.backward(error)\n",
        "\n",
        "    def save_weights(self, file_name):\n",
        "        \"\"\"\n",
        "        Armazena os pesos das camadas num arquivo.\n",
        "\n",
        "        Args:\n",
        "            file_name (string): Nome do arquivo que irá armazenar os valores.\n",
        "        \"\"\"\n",
        "        # Criando um dicionário para salvar os pesos de todas as camadas\n",
        "        network = {\n",
        "            # Conversão dos pesos da primeira camada oculta para uma lista\n",
        "            'hidden_layer1': {'weights': self.hidden_layer1.weights.tolist(),\n",
        "                            'biases': self.hidden_layer1.biases.tolist()},\n",
        "\n",
        "            # Conversão dos pesos da camada de saída para uma lista\n",
        "            'output_layer': {'weights': self.output_layer.weights.tolist(),\n",
        "                            'biases': self.output_layer.biases.tolist()}\n",
        "        }\n",
        "\n",
        "        # Abertura do arquivo para escrita\n",
        "        with open(file_name, 'w') as f:\n",
        "            # Salvando o dicionário no fomrato JSON\n",
        "            json.dump(network, f) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yQ1pTuonxJnC"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron:\n",
        "    def __init__(self, min_output_error, max_epochs_num, early_stop_param, learning_rate):\n",
        "        \"\"\"\n",
        "        Inicializa os parâmetros para o treinamento da rede neural.\n",
        "\n",
        "        Args:\n",
        "            min_output_error (float): Erro mínimo desejado para o treinamento.\n",
        "            max_epochs_num (int): Número máximo de épocas para o treinamento.\n",
        "            early_stop_param (int): Número de épocas consecutivas com aumento do erro permitido antes da parada antecipada.\n",
        "            learning_rate (float): Taxa de aprendizado do modelo.\n",
        "\n",
        "        Attributes:\n",
        "            min_output_error (float): Erro mínimo desejado para o treinamento.\n",
        "            max_epochs_num (int): Número máximo de épocas para o treinamento.\n",
        "            early_stop_param (int): Número de épocas consecutivas com aumento do erro permitido antes da parada antecipada.\n",
        "            negative_error_var_num (int): Contador de épocas com aumento do erro.\n",
        "            current_epoch_error (float): Taxa de erro da época atual.\n",
        "            last_epoch_error (float): Taxa de erro da última época.\n",
        "            neural_network (NeuralNetwork): Rede neural do modelo.\n",
        "        \"\"\"\n",
        "        self.min_output_error = min_output_error\n",
        "        self.max_epochs_num = max_epochs_num\n",
        "        self.early_stop_param = early_stop_param\n",
        "        self.learning_rate = learning_rate\n",
        "        self.negative_error_var_num = 0\n",
        "        self.current_epoch_error = 0\n",
        "        self.last_epoch_error = 0\n",
        "        self.neural_network = None\n",
        "\n",
        "        # Lista para armazenar os erros de treinamento em cada época\n",
        "        self.training_errors = []\n",
        "\n",
        "        # Inicializa a rede neural\n",
        "        neural_network = NeuralNetwork(learning_rate=0.01)\n",
        "        self.neural_network = neural_network\n",
        "\n",
        "    def train(self, entrada, classes, earlystop=False):\n",
        "        \"\"\"\n",
        "        Treina a rede neural usando o conjunto de dados de treinamento.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Dados de entrada para treinamento.\n",
        "            classes (np.array): Labels correspondentes para o treinamento.\n",
        "            earlystop (boolean): Booleano que indica se earlystop será usado ou não\n",
        "        \"\"\"\n",
        "        # Divide os dados de entrada num conjunto de treinamento e outro de validação\n",
        "        x_train, y_train, x_val, y_val = dividir_dados(entrada, classes)\n",
        "        \n",
        "        # Treina a MLP\n",
        "        for epoch in range(self.max_epochs_num):\n",
        "            epoch_error = 0\n",
        "\n",
        "            # Loop que treina a MLP com os dados de treinamento\n",
        "            for x, y in zip(x_train, y_train):\n",
        "                output = self.neural_network.forward_propagation(x)\n",
        "                output_error = y - output\n",
        "                self.neural_network.back_propagation(output_error)\n",
        "                epoch_error += np.mean(np.abs(output_error))\n",
        "\n",
        "            train_error = epoch_error / x_train.shape[0]\n",
        "\n",
        "            # Calcula a acurácia e erro de validação do modelo\n",
        "            acuracia, val_error = self.get_accuracy(x_val, y_val)\n",
        "\n",
        "            self.training_errors.append(val_error)\n",
        "\n",
        "            # Printa erro e acurácia a cada dez épocas\n",
        "            if epoch % 10 == 0:\n",
        "                print(\n",
        "                    f\"Época {epoch}/{self.max_epochs_num}, Acurácia: {acuracia}, Erro de validação: {val_error}, Erro de treino: {train_error}\")\n",
        "            \n",
        "            # Verifica parâmetros de earlystop, caso esteja sendo usada\n",
        "            if earlystop:\n",
        "                if train_error < self.min_output_error:\n",
        "                    print(\"Erro mínimo atingido. Parando o treinamento.\")\n",
        "                    break\n",
        "                \n",
        "                # Verifica se o erro aumentou\n",
        "                if epoch > 0 and self.training_errors[epoch] > self.training_errors[epoch - 1]:\n",
        "                    self.negative_error_var_num += 1\n",
        "                else:\n",
        "                    self.negative_error_var_num = 0\n",
        "\n",
        "                if self.negative_error_var_num >= self.early_stop_param:\n",
        "                    print(\"Parando antecipadamente devido ao aumento contínuo do erro.\")\n",
        "                    break\n",
        "        \n",
        "        self.neural_network.save_weights(\"pesos_mlp.txt\")\n",
        "    \n",
        "    def get_accuracy(self, entrada, saida_esperada):\n",
        "        \"\"\"\n",
        "        Obtém a acurácia da MLP.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Dados de entrada para teste.\n",
        "            saida_esperada (np.array): Saída esperada da rede neural.\n",
        "\n",
        "        Returns:\n",
        "            acuracia (float): Porcentagem de acertos da rede.\n",
        "        \"\"\"\n",
        "\n",
        "        # Inicializa variáveis para calcular acurácia\n",
        "        num_testes = 0\n",
        "        num_acertos = 0\n",
        "        erro = 0\n",
        "\n",
        "        # Realiza teste para cada um dos dados de entrada\n",
        "        for x, y in zip(entrada, saida_esperada):\n",
        "            num_testes += 1\n",
        "            \n",
        "            # Realiza a predição\n",
        "            predicao = self.neural_network.forward_propagation(x)\n",
        "\n",
        "            # Calcula erro\n",
        "            output_error = y - predicao\n",
        "            erro += np.mean(np.abs(output_error))\n",
        "\n",
        "            # Obtém a letra predita pelo modelo\n",
        "            letra_predita = vetor_para_letra(predicao)\n",
        "\n",
        "            # Obtém a letra esperada\n",
        "            letra_real = vetor_para_letra(y)\n",
        "\n",
        "            if letra_predita == letra_real:\n",
        "                num_acertos += 1\n",
        "\n",
        "        \n",
        "        acuracia = num_acertos / num_testes\n",
        "        erro_medio = erro / entrada.shape[0]\n",
        "\n",
        "        return acuracia, erro_medio\n",
        "\n",
        "    def predict(self, entrada):\n",
        "        \"\"\"\n",
        "        Realiza predições usando a rede neural treinada.\n",
        "\n",
        "        Args:\n",
        "            entrada (np.array): Dados de entrada para teste.\n",
        "            neural_network (NeuralNetwork): Instância da rede neural treinada.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Predições da rede neural.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x in entrada:\n",
        "            output = self.neural_network.forward_propagation(x)\n",
        "            predictions.append(output)\n",
        "        return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Época 0/100, Acurácia: 0.23015873015873015, Erro de validação: 0.1511737213068711, Erro de treino: 0.2842970637885704\n",
            "Época 10/100, Acurácia: 0.7619047619047619, Erro de validação: 0.1718883037623602, Erro de treino: 0.1592362233177095\n",
            "Época 20/100, Acurácia: 0.8650793650793651, Erro de validação: 0.18071916781497438, Erro de treino: 0.15223369713555387\n",
            "Época 30/100, Acurácia: 0.8452380952380952, Erro de validação: 0.17626421371886622, Erro de treino: 0.14235449572789682\n",
            "Época 40/100, Acurácia: 0.8611111111111112, Erro de validação: 0.1624336039974571, Erro de treino: 0.13603818351668462\n",
            "Época 50/100, Acurácia: 0.8690476190476191, Erro de validação: 0.16630906630034248, Erro de treino: 0.13119032001473857\n",
            "Época 60/100, Acurácia: 0.8373015873015873, Erro de validação: 0.16412234855229824, Erro de treino: 0.12466244192247215\n",
            "Época 70/100, Acurácia: 0.873015873015873, Erro de validação: 0.1598593148921894, Erro de treino: 0.12236332677632633\n",
            "Época 80/100, Acurácia: 0.8531746031746031, Erro de validação: 0.157884269096685, Erro de treino: 0.11709089246814798\n",
            "Época 90/100, Acurácia: 0.8611111111111112, Erro de validação: 0.15657230448835927, Erro de treino: 0.11701469757144696\n",
            "Treinamento concluído.\n",
            "Acurácia do modelo: 0.7910447761194029\n",
            "Erro médio do modelo: 0.17838709726592777\n",
            "Predição: B, Real: B\n",
            "Predição: G, Real: C\n",
            "Predição: N, Real: N\n",
            "Predição: K, Real: K\n",
            "Predição: B, Real: B\n",
            "Predição: G, Real: G\n",
            "Predição: I, Real: J\n",
            "Predição: N, Real: W\n",
            "Predição: I, Real: I\n",
            "Predição: V, Real: V\n",
            "Predição: H, Real: H\n",
            "Predição: D, Real: D\n",
            "Predição: B, Real: B\n",
            "Predição: U, Real: U\n",
            "Predição: D, Real: D\n",
            "Predição: G, Real: O\n",
            "Predição: M, Real: M\n",
            "Predição: P, Real: P\n",
            "Predição: O, Real: D\n",
            "Predição: H, Real: H\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Lendo os arquivos\n",
        "    imagens = ler_arq_imagens('X.txt')\n",
        "    classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "    # Dividindo os dados em conjuntos de treinamento e teste\n",
        "    x_train, y_train, x_test, y_test = dividir_dados(imagens, classes, test_proportion=0.05)\n",
        "\n",
        "    # Convertendo para float\n",
        "    y_train = y_train.astype(float)\n",
        "    y_test = y_test.astype(float)\n",
        "\n",
        "    # Parâmetros para o treinamento\n",
        "    min_output_error = 0.001\n",
        "    max_epochs_num = 1000\n",
        "    early_stop_param = 30\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    # Inicializa o Multilayer Perceptron com os parâmetros de treinamento\n",
        "    mlp = MultilayerPerceptron(min_output_error, max_epochs_num, early_stop_param, learning_rate)\n",
        "\n",
        "    # Treina a rede neural\n",
        "    mlp.train(x_train, y_train)\n",
        "\n",
        "    print(\"Treinamento concluído.\")\n",
        "\n",
        "    acuracia, erro = mlp.get_accuracy(x_test, y_test)\n",
        "    print(f\"Acurácia do modelo: {acuracia}\")\n",
        "    print(f\"Erro médio do modelo: {erro}\")\n",
        "\n",
        "    # Testa a rede neural\n",
        "    predictions = mlp.predict(x_test)\n",
        "\n",
        "    # Exibe algumas predições para verificação\n",
        "    num_testes = 0\n",
        "    acertos = 0\n",
        "    for i in range(20):\n",
        "        num_testes += 1\n",
        "\n",
        "        predicao = vetor_para_letra(predictions[i])\n",
        "        correto = vetor_para_letra(y_test[i])\n",
        "\n",
        "        if predicao == correto:\n",
        "            acertos += 1\n",
        "        print(f\"Predição: {predicao}, Real: {correto}\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kfold(entrada, classes, num_particoes):\n",
        "    \"\"\"\n",
        "    Divide os dados e classes em K partições para K-Fold Cross-Validation.\n",
        "\n",
        "    Args:\n",
        "        entrada (np.array): Vetor dos dados a serem divididos.\n",
        "        classes (np.array): Vetor das classes dos dados.\n",
        "        num_particoes (int): Número de partições (K).\n",
        "\n",
        "    Returns:\n",
        "        dados_particionados (list): Lista de K partições dos dados.\n",
        "        classes_particionadas (list): Lista de K partições das classes.\n",
        "    \"\"\"\n",
        "    tam_entrada = entrada.shape[0]\n",
        "    tam_particoes = int(tam_entrada / num_particoes)\n",
        "    particoes = []\n",
        "\n",
        "    # Embaralha os índices para os dados serem divididos de forma aleatória\n",
        "    indices = np.arange(tam_entrada)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Divide o vetor de índice em K partes\n",
        "    for i in range(num_particoes):\n",
        "        if i != num_particoes - 1:\n",
        "            particoes.append(indices[(i * tam_particoes) : ((i + 1) * tam_particoes)])\n",
        "        else:\n",
        "            particoes.append(indices[(i * tam_particoes) :])\n",
        "\n",
        "    dados_particionados = []\n",
        "    classes_particionadas = []\n",
        "\n",
        "    # Define os vetores de dados e classificação de cada partição\n",
        "    for p in particoes:\n",
        "        dados_particionados.append(entrada[p])\n",
        "        classes_particionadas.append(classes[p])\n",
        "\n",
        "    return dados_particionados, classes_particionadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1326, 120)\n",
            "(1326, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(265, 120)\n",
            "(265, 26)\n",
            "(266, 120)\n",
            "(266, 26)\n"
          ]
        }
      ],
      "source": [
        "# Teste função K-Fold\n",
        "imagens = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "print(imagens.shape)\n",
        "print(classes.shape)\n",
        "\n",
        "imagens_part, classes_part = kfold(imagens, classes, 5)\n",
        "\n",
        "for i in range(5):\n",
        "    print(imagens_part[i].shape)\n",
        "    print(classes_part[i].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinamento K-Fold 1: \n",
            "Época 0/300, Acurácia: 0.1643192488262911, Erro: 299.6694117446212\n",
            "Época 30/300, Acurácia: 0.8450704225352113, Erro: 127.2868908567015\n",
            "Época 60/300, Acurácia: 0.863849765258216, Erro: 116.24617085003628\n",
            "Época 90/300, Acurácia: 0.8544600938967136, Erro: 103.32289961961185\n",
            "Época 120/300, Acurácia: 0.863849765258216, Erro: 97.14766297671109\n",
            "Época 150/300, Acurácia: 0.8497652582159625, Erro: 93.51296021003299\n",
            "Época 180/300, Acurácia: 0.8403755868544601, Erro: 89.38594217891062\n",
            "Época 210/300, Acurácia: 0.8591549295774648, Erro: 85.9362954934893\n",
            "Época 240/300, Acurácia: 0.863849765258216, Erro: 86.09945449223862\n",
            "Época 270/300, Acurácia: 0.863849765258216, Erro: 83.28318696020732\n",
            "Treinamento 1 finalizado.\n",
            "Acurácia: 0.84\n",
            "\n",
            "Treinamento K-Fold 2: \n",
            "Época 0/300, Acurácia: 0.14084507042253522, Erro: 240.81156741086252\n",
            "Época 30/300, Acurácia: 0.7981220657276995, Erro: 124.510221243374\n",
            "Época 60/300, Acurácia: 0.8356807511737089, Erro: 106.76017868022201\n",
            "Época 90/300, Acurácia: 0.8262910798122066, Erro: 95.78554537318442\n",
            "Época 120/300, Acurácia: 0.8169014084507042, Erro: 88.74402932981278\n",
            "Época 150/300, Acurácia: 0.8215962441314554, Erro: 83.29594793330138\n",
            "Época 180/300, Acurácia: 0.8403755868544601, Erro: 81.6469249361923\n",
            "Época 210/300, Acurácia: 0.8309859154929577, Erro: 76.09843243351509\n",
            "Época 240/300, Acurácia: 0.8544600938967136, Erro: 75.99703061685707\n",
            "Época 270/300, Acurácia: 0.8309859154929577, Erro: 73.65862416495796\n",
            "Treinamento 2 finalizado.\n",
            "Acurácia: 0.83\n",
            "\n",
            "Treinamento K-Fold 3: \n",
            "Época 0/300, Acurácia: 0.15023474178403756, Erro: 299.233513229742\n",
            "Época 30/300, Acurácia: 0.8262910798122066, Erro: 128.27919600027232\n",
            "Época 60/300, Acurácia: 0.8450704225352113, Erro: 112.05112778106732\n",
            "Época 90/300, Acurácia: 0.8356807511737089, Erro: 100.86132990891645\n",
            "Época 120/300, Acurácia: 0.8450704225352113, Erro: 95.41419478258281\n",
            "Época 150/300, Acurácia: 0.8497652582159625, Erro: 91.72989860410952\n",
            "Época 180/300, Acurácia: 0.8356807511737089, Erro: 89.17377251092509\n",
            "Época 210/300, Acurácia: 0.8356807511737089, Erro: 87.63122120188385\n",
            "Época 240/300, Acurácia: 0.8497652582159625, Erro: 87.09002328697345\n",
            "Época 270/300, Acurácia: 0.8544600938967136, Erro: 85.26799595033779\n",
            "Treinamento 3 finalizado.\n",
            "Acurácia: 0.83\n",
            "\n",
            "Treinamento K-Fold 4: \n",
            "Época 0/300, Acurácia: 0.14084507042253522, Erro: 265.8501331675709\n",
            "Época 30/300, Acurácia: 0.863849765258216, Erro: 123.36109471743015\n",
            "Época 60/300, Acurácia: 0.8544600938967136, Erro: 111.90045862822151\n",
            "Época 90/300, Acurácia: 0.8544600938967136, Erro: 102.01532445431023\n",
            "Época 120/300, Acurácia: 0.8685446009389671, Erro: 91.80398983705533\n",
            "Época 150/300, Acurácia: 0.8685446009389671, Erro: 87.03379295370739\n",
            "Época 180/300, Acurácia: 0.8591549295774648, Erro: 91.04605332129077\n",
            "Época 210/300, Acurácia: 0.8732394366197183, Erro: 84.8440482566381\n",
            "Época 240/300, Acurácia: 0.8732394366197183, Erro: 80.23763339202759\n",
            "Época 270/300, Acurácia: 0.863849765258216, Erro: 78.5840764692736\n",
            "Treinamento 4 finalizado.\n",
            "Acurácia: 0.84\n",
            "\n",
            "Treinamento K-Fold 5: \n",
            "Época 0/300, Acurácia: 0.1179245283018868, Erro: 253.94786387393202\n",
            "Época 30/300, Acurácia: 0.8207547169811321, Erro: 125.61248458987905\n",
            "Época 60/300, Acurácia: 0.8584905660377359, Erro: 109.61372527666155\n",
            "Época 90/300, Acurácia: 0.8773584905660378, Erro: 98.45686710629006\n",
            "Época 120/300, Acurácia: 0.8867924528301887, Erro: 93.20428010408206\n",
            "Época 150/300, Acurácia: 0.8679245283018868, Erro: 85.40773052186903\n",
            "Época 180/300, Acurácia: 0.8632075471698113, Erro: 84.95115848143554\n",
            "Época 210/300, Acurácia: 0.8726415094339622, Erro: 83.1675586682334\n",
            "Época 240/300, Acurácia: 0.8773584905660378, Erro: 78.41086307209135\n",
            "Época 270/300, Acurácia: 0.8820754716981132, Erro: 75.43196997978099\n",
            "Treinamento 5 finalizado.\n",
            "Acurácia: 0.85\n",
            "\n",
            "Cross-validation concluído.\n",
            "Acurácia média: 0.8378465030500781\n"
          ]
        }
      ],
      "source": [
        "def train_cross_validation(entrada, classes, num_particoes):\n",
        "    \n",
        "    particoes_dados, particoes_classes = kfold(entrada, classes, num_particoes)\n",
        "\n",
        "    min_output_error = 0.01\n",
        "    max_epochs_num = 300\n",
        "    early_stop_param = 15\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    acuracia_total = 0\n",
        "\n",
        "    for i in range(num_particoes):\n",
        "        print(f\"Treinamento K-Fold {i + 1}: \")\n",
        "\n",
        "        # Define conjunto de testes\n",
        "        dados_teste = particoes_dados[i]\n",
        "        classes_teste = particoes_classes[i]\n",
        "\n",
        "        # Remove conjunto de testes das partições\n",
        "        min_output_error, max_epochs_num, early_stop_param, learning_rate\n",
        "        dados_treinamento = [particao for j, particao in enumerate(particoes_dados) if j != i]\n",
        "        dados_treinamento = np.concatenate(dados_treinamento)\n",
        "        classes_treinamento = [particao for j, particao in enumerate(particoes_classes) if j != i]\n",
        "        classes_treinamento = np.concatenate(classes_treinamento)\n",
        "\n",
        "        # Inicia MLP\n",
        "        mlp = MultilayerPerceptron(min_output_error, max_epochs_num, early_stop_param, learning_rate)\n",
        "\n",
        "        # Treina MLP\n",
        "        mlp.train(dados_treinamento, classes_treinamento)\n",
        "\n",
        "        print(f\"Treinamento {i + 1} finalizado.\")\n",
        "        acuracia = mlp.get_accuracy(dados_teste, classes_teste)\n",
        "        print(f\"Acurácia: {acuracia:.2f}\\n\")\n",
        "\n",
        "        acuracia_total += acuracia\n",
        "    \n",
        "    print(\"Cross-validation concluído.\")\n",
        "    print(f\"Acurácia média: {acuracia_total / num_particoes:.2f}\")\n",
        "\n",
        "# Lendo os arquivos\n",
        "imagens = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes('Y_letra.txt')\n",
        "\n",
        "# Convertendo para float\n",
        "classes = classes.astype(float)\n",
        "\n",
        "train_cross_validation(imagens, classes, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6wKX05Ojl8W"
      },
      "source": [
        "## Exemplos de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhSXY_Wzsrnf"
      },
      "outputs": [],
      "source": [
        "imagens = ler_arq_imagens('X.txt')\n",
        "classes = ler_arq_classes('Y_letra.txt')\n",
        "print(imagens.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classes.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executando o treinamento e teste da rede neural\n",
        "mlp = MultilayerPerceptron()\n",
        "mlp.main(imagens, classes)#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotando o erro de treinamento\n",
        "plt.plot(mlp.training_errors)\n",
        "plt.title('Erro de Treinamento por Época')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Erro')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criação de uma tabela com os erros de treinamento\n",
        "df_erros_treinamento = pd.DataFrame({\n",
        "    'Época': range(len(mlp.training_errors)),\n",
        "    'Erro': mlp.training_errors\n",
        "})\n",
        "\n",
        "# Exibindo a tabela\n",
        "print(df_erros_treinamento)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Falta:\n",
        "\n",
        "- fazer o erro parar de diminuir pouco\n",
        "- tirar sklearn do cross-validation\n",
        "- verificar se o gráfico tá gerando certo\n",
        "- gerar matriz de confusão para testes da rede\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Roteiro para vídeo\n",
        "\n",
        "V1:\n",
        "- Parte de treinamento do código (com e sem validação cruzada e parada antecipada) PARA: conjunto de dados CARACTERES COMPLETO (os outros de teste não entram)\n",
        "- Estudo dos parâmetros (busca por valores adequados – grid) (?)\n",
        "- Teste da MLP para o conjunto de dados CARACTERES COMPLETO\n",
        "\n",
        "V2:\n",
        "- \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
